{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulLnhuwU5P5u"
      },
      "source": [
        "IMPORTING ALL REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CHyBvLb-5P51"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import json\n",
        "import calendar as cal\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "fB7hpsDg5P53",
        "outputId": "4d943492-e28e-4ff9-8589-2f1229cc281b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\191037044.py:15: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "added chunk\n",
            "added chunk\n",
            "added chunk\n",
            "added chunk\n",
            "added chunk\n",
            "added chunk\n",
            "added chunk\n",
            "Processing data...\n"
          ]
        }
      ],
      "source": [
        "chunk_size = 100000\n",
        "\n",
        "# Initialize an empty list to store the chunks\n",
        "processed_data = []\n",
        "\n",
        "# Path to the zip file and the CSV file inside it\n",
        "zip_path = r\"consult_det_report_gen.zip\"\n",
        "csv_filename = \"consult_det_report_gen.csv\"\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    # Open the CSV file inside the zip file\n",
        "    with z.open(csv_filename) as f:\n",
        "        # Read the specified rows in chunks from the CSV file\n",
        "        for chunk in pd.read_csv(\n",
        "            f,\n",
        "            sep=\"|\",\n",
        "            encoding=\"windows-1252\",\n",
        "            encoding_errors=\"ignore\",\n",
        "            on_bad_lines=\"skip\",\n",
        "            chunksize=chunk_size,\n",
        "        ):\n",
        "            print(\"added chunk\")\n",
        "            processed_data.append(chunk)\n",
        "\n",
        "    # Reset the pointer to the beginning of the CSV file for the header read\n",
        "    with z.open(csv_filename) as f:\n",
        "        headers = pd.read_csv(f, nrows=1, sep=\"|\")\n",
        "\n",
        "# Concatenate the chunks into a single DataFrame\n",
        "processed_df_1 = pd.concat(processed_data)\n",
        "processed_df_1.columns = headers.columns\n",
        "print(\"Processing data...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9h8ZC-Ov5P55",
        "outputId": "91b6e312-ef02-4823-e074-590a01bec1e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Product</th>\n",
              "      <th>Channel</th>\n",
              "      <th>EMR#</th>\n",
              "      <th>Consultation #</th>\n",
              "      <th>Mobile #</th>\n",
              "      <th>Member #</th>\n",
              "      <th>Type</th>\n",
              "      <th>Status</th>\n",
              "      <th>Patient Gender</th>\n",
              "      <th>...</th>\n",
              "      <th>Doctor</th>\n",
              "      <th>Chief Complaint</th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>Advice</th>\n",
              "      <th>REFERRED</th>\n",
              "      <th>Lab Tests</th>\n",
              "      <th>Medicine</th>\n",
              "      <th>Clinic Name</th>\n",
              "      <th>Prescrption json</th>\n",
              "      <th>ICD Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-05-15</td>\n",
              "      <td>M-Swasth  Doctor - FREE</td>\n",
              "      <td>CashPor</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7571029662</td>\n",
              "      <td>21:01:84:01:25</td>\n",
              "      <td>CALL</td>\n",
              "      <td>DONE</td>\n",
              "      <td>M</td>\n",
              "      <td>...</td>\n",
              "      <td>SATISH</td>\n",
              "      <td>Other</td>\n",
              "      <td>.</td>\n",
              "      <td>referred to higher centre</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{\"pres\":[]}</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-06-10</td>\n",
              "      <td>M-Swasth  Doctor - FREE</td>\n",
              "      <td>CashPor</td>\n",
              "      <td>1</td>\n",
              "      <td>47576</td>\n",
              "      <td>7571029662</td>\n",
              "      <td>21:01:84:01:25</td>\n",
              "      <td>CALL</td>\n",
              "      <td>DONE</td>\n",
              "      <td>M</td>\n",
              "      <td>...</td>\n",
              "      <td>KADAMBARI</td>\n",
              "      <td>Other</td>\n",
              "      <td>Dysmenorrhea and menorrhagia</td>\n",
              "      <td>Advised not miss a single tablet of Ovral l\\nD...</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>YES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{\"pres\":[{\"mName\":\"Tab Ovral L\",\"m\":\"\",\"a\":\"\",...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-06-11</td>\n",
              "      <td>M-Swasth  Doctor - FREE</td>\n",
              "      <td>CashPor</td>\n",
              "      <td>1</td>\n",
              "      <td>48043</td>\n",
              "      <td>7571029662</td>\n",
              "      <td>21:01:84:01:25</td>\n",
              "      <td>CALL</td>\n",
              "      <td>DONE</td>\n",
              "      <td>M</td>\n",
              "      <td>...</td>\n",
              "      <td>NIKHIL</td>\n",
              "      <td>Other</td>\n",
              "      <td>C/o Weakness with loss of appetite</td>\n",
              "      <td>Eat healthy food , green vegetables and fruits...</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>YES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{\"pres\":[{\"mName\":\"Multiprex Cap\",\"m\":\"1\",\"a\":...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-06-11</td>\n",
              "      <td>M-Swasth  Doctor - FREE</td>\n",
              "      <td>CashPor</td>\n",
              "      <td>1</td>\n",
              "      <td>48603</td>\n",
              "      <td>7571029662</td>\n",
              "      <td>21:01:84:01:25</td>\n",
              "      <td>CALL</td>\n",
              "      <td>DONE</td>\n",
              "      <td>M</td>\n",
              "      <td>...</td>\n",
              "      <td>ravindra</td>\n",
              "      <td>Other</td>\n",
              "      <td>pain</td>\n",
              "      <td>Take nutritious diet\\nDo Excercise regularly\\n...</td>\n",
              "      <td>NO</td>\n",
              "      <td>CBC</td>\n",
              "      <td>YES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{\"pres\":[{\"mName\":\"Paracetamol 500mg Tab\",\"m\":...</td>\n",
              "      <td>R50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-06-16</td>\n",
              "      <td>M-Swasth  Doctor - FREE</td>\n",
              "      <td>CashPor</td>\n",
              "      <td>1</td>\n",
              "      <td>52282</td>\n",
              "      <td>7571029662</td>\n",
              "      <td>21:01:84:01:25</td>\n",
              "      <td>CALL</td>\n",
              "      <td>DONE</td>\n",
              "      <td>M</td>\n",
              "      <td>...</td>\n",
              "      <td>Harinder Pal</td>\n",
              "      <td>Other Pain</td>\n",
              "      <td>Gall Stones</td>\n",
              "      <td>Referred to nearby hospital for surgical consu...</td>\n",
              "      <td>YES</td>\n",
              "      <td>Ultrasound</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{\"pres\":[]}</td>\n",
              "      <td>K87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date                  Product  Channel EMR# Consultation #  \\\n",
              "0  2020-05-15  M-Swasth  Doctor - FREE  CashPor    1              1   \n",
              "1  2021-06-10  M-Swasth  Doctor - FREE  CashPor    1          47576   \n",
              "2  2021-06-11  M-Swasth  Doctor - FREE  CashPor    1          48043   \n",
              "3  2021-06-11  M-Swasth  Doctor - FREE  CashPor    1          48603   \n",
              "4  2021-06-16  M-Swasth  Doctor - FREE  CashPor    1          52282   \n",
              "\n",
              "     Mobile #        Member #  Type Status Patient Gender  ...        Doctor  \\\n",
              "0  7571029662  21:01:84:01:25  CALL   DONE              M  ...        SATISH   \n",
              "1  7571029662  21:01:84:01:25  CALL   DONE              M  ...     KADAMBARI   \n",
              "2  7571029662  21:01:84:01:25  CALL   DONE              M  ...        NIKHIL   \n",
              "3  7571029662  21:01:84:01:25  CALL   DONE              M  ...      ravindra   \n",
              "4  7571029662  21:01:84:01:25  CALL   DONE              M  ...  Harinder Pal   \n",
              "\n",
              "  Chief Complaint                           Diagnosis  \\\n",
              "0           Other                                   .   \n",
              "1           Other        Dysmenorrhea and menorrhagia   \n",
              "2           Other  C/o Weakness with loss of appetite   \n",
              "3           Other                                pain   \n",
              "4      Other Pain                         Gall Stones   \n",
              "\n",
              "                                              Advice REFERRED   Lab Tests  \\\n",
              "0                          referred to higher centre       NO         NaN   \n",
              "1  Advised not miss a single tablet of Ovral l\\nD...       NO         NaN   \n",
              "2  Eat healthy food , green vegetables and fruits...       NO         NaN   \n",
              "3  Take nutritious diet\\nDo Excercise regularly\\n...       NO         CBC   \n",
              "4  Referred to nearby hospital for surgical consu...      YES  Ultrasound   \n",
              "\n",
              "  Medicine Clinic Name                                   Prescrption json  \\\n",
              "0       NO         NaN                                        {\"pres\":[]}   \n",
              "1      YES         NaN  {\"pres\":[{\"mName\":\"Tab Ovral L\",\"m\":\"\",\"a\":\"\",...   \n",
              "2      YES         NaN  {\"pres\":[{\"mName\":\"Multiprex Cap\",\"m\":\"1\",\"a\":...   \n",
              "3      YES         NaN  {\"pres\":[{\"mName\":\"Paracetamol 500mg Tab\",\"m\":...   \n",
              "4       NO         NaN                                        {\"pres\":[]}   \n",
              "\n",
              "  ICD Code  \n",
              "0        .  \n",
              "1     NONE  \n",
              "2     NONE  \n",
              "3      R50  \n",
              "4      K87  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b4qQ7VU15P56",
        "outputId": "1a00b59d-dd71-4b8e-9832-b7695f6882d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4990682 entries, 0 to 4990681\n",
            "Data columns (total 21 columns):\n",
            " #   Column            Dtype \n",
            "---  ------            ----- \n",
            " 0   Date              object\n",
            " 1   Product           object\n",
            " 2   Channel           object\n",
            " 3   EMR#              object\n",
            " 4   Consultation #    object\n",
            " 5   Mobile #          object\n",
            " 6   Member #          object\n",
            " 7   Type              object\n",
            " 8   Status            object\n",
            " 9   Patient Gender    object\n",
            " 10  FEMALE DOC ONLY   object\n",
            " 11  Doctor            object\n",
            " 12  Chief Complaint   object\n",
            " 13  Diagnosis         object\n",
            " 14  Advice            object\n",
            " 15  REFERRED          object\n",
            " 16  Lab Tests         object\n",
            " 17  Medicine          object\n",
            " 18  Clinic Name       object\n",
            " 19  Prescrption json  object\n",
            " 20  ICD Code          object\n",
            "dtypes: object(21)\n",
            "memory usage: 799.6+ MB\n"
          ]
        }
      ],
      "source": [
        "processed_df_1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pL6b3X6Y5P57",
        "outputId": "bd5a9be8-fee9-418f-c1e7-6c8a11d442f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Channel\n",
              "CashPor                                1896985\n",
              "Muthoot                                 860034\n",
              "Svatantra                               549417\n",
              "Namra Finance                           446609\n",
              "Utkarsh SFB                             330605\n",
              "                                        ...   \n",
              "Adhikosh Financial Advisory Pvt Ltd          3\n",
              "Aishwarya MACC Society Ltd                   2\n",
              "Sudhir HUF                                   1\n",
              "CBC                                          1\n",
              "North East Small Finance Bank - SHG          1\n",
              "Name: count, Length: 70, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df_1['Channel'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iogr0cnH5P58",
        "outputId": "8e0f37c7-7494-4017-a723-99e0a0e691f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Namra Finance']\n",
            "Channels: ['CashPor', 'Muthoot', 'Svatantra', 'Namra Finance', 'Utkarsh SFB', 'TATA Capital', 'North East Small Finance Bank', 'Pahal Finance', 'Adi Chitragupta Finance Limited', 'Bangia Gramin Vikas Bank', 'Uttrayan', 'Satya Microcapital', 'Humana Financial Services', 'VFS', 'Swarnodhayam Credits Pvt Ltd', 'NABFINS Limited', 'Paschim Banga Gramin Bank', 'Share India Fincap', 'SEBA RAHARA', 'Uttar Banga Kshetriya Garmin Bank', 'Annapurna', 'Bank of India', 'Uttar Bihar Gramin Bank', 'Servitium Micro Finance Pvt Ltd.', 'Sonata Finance', 'ESAF HEALTH CARE SERVICES PVT. LTD', 'CCFID', 'Mitrata ', 'M-INSURE CHANNEL', 'Maharashtra Gramin Bank', 'ASA International', 'Arohan Financial Services Limited', 'APGVB', 'Sugamya', 'Midland Microfin', 'Chhattisgarh Rajya Gramin Bank', 'Sampurna Training and Entrepreneurship Programme', 'DCBS', 'Hirbandh Ramkrishna Sarada Sevashrama', 'Indiabulls Housing Finance ', 'Sindhuja Microcredit', 'Muthoot Capital', 'Sarala Development & Microfinance Pvt. Ltd.', 'Madhyanchal Gramin Bank', 'Baghat Urban Co-op Bank Ltd', \"People's Forum\", 'KBS Local Bank', 'Janakalyan', 'Your Global Money', 'Agora Microfinance', 'Ambitions Services', 'Kamal Fincap Private Limited', 'Maddox Consultancy Services Pvt. Ltd.', 'AISSHPRA', 'Rohtak Central Cooperative Bank', 'Projects', ' Swabhimaan Sales & Services Pvt. Ltd.', 'South India Finvest Pvt Ltd', 'Panchawati', 'Pune District Central Co-op bank', 'APGVB Pensioners Association', ' IFFCO Bazar', 'Aviral Finance', 'Shepays Financial Services Pvt Ltd.', 'DULAL NGO', 'Adhikosh Financial Advisory Pvt Ltd', 'Aishwarya MACC Society Ltd', 'Sudhir HUF', 'CBC', 'North East Small Finance Bank - SHG']\n",
            "Counts: [1896985, 860034, 549417, 446609, 330605, 194492, 159403, 117868, 104095, 86132, 69664, 46411, 15084, 12610, 11314, 10279, 9521, 6825, 5818, 5377, 5300, 5155, 4627, 4253, 3420, 3000, 2929, 2806, 2312, 2193, 2063, 1483, 1127, 1116, 1090, 1053, 971, 898, 881, 689, 530, 498, 417, 398, 365, 353, 336, 326, 237, 169, 168, 150, 142, 107, 95, 89, 68, 66, 60, 58, 47, 32, 28, 18, 5, 3, 2, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "channel_value_counts = processed_df_1['Channel'].value_counts()\n",
        "\n",
        "channels_list = channel_value_counts.index.tolist()\n",
        "channels=channels_list[3:4]\n",
        "print(channels)\n",
        "\n",
        "counts_list = channel_value_counts.values.tolist()\n",
        "\n",
        "print(\"Channels:\", channels_list)\n",
        "print(\"Counts:\", counts_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15bu_565P58"
      },
      "source": [
        "FUNCTIONS BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8xQrpTkZ5P59"
      },
      "outputs": [],
      "source": [
        "def extract_medication_details(prescription, clinic, date):\n",
        "    med_details = []\n",
        "    if isinstance(prescription, str):\n",
        "        try:\n",
        "            prescriptions = json.loads(prescription)[\"pres\"]\n",
        "            for prescription in prescriptions:\n",
        "                med_name = prescription.get(\"mName\", \"\")\n",
        "                duration = prescription.get(\"dur\", \"\")\n",
        "                morning = 1 if prescription.get(\"m\") else 0\n",
        "                afternoon = 1 if prescription.get(\"a\") else 0\n",
        "                evening = 1 if prescription.get(\"e\") else 0\n",
        "                night = 1 if prescription.get(\"n\") else 0\n",
        "                frequency = prescription.get(\"freq\", \"\")\n",
        "                med_details.append(\n",
        "                    (\n",
        "                        med_name,\n",
        "                        duration,\n",
        "                        morning,\n",
        "                        afternoon,\n",
        "                        evening,\n",
        "                        night,\n",
        "                        frequency,\n",
        "                        clinic,\n",
        "                        date,\n",
        "                    )\n",
        "                )\n",
        "        except (json.JSONDecodeError, KeyError):\n",
        "            pass  # Skip non-JSON entries or entries with missing keys\n",
        "    return med_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Borg8d_S5P5-"
      },
      "outputs": [],
      "source": [
        "def abc(data):\n",
        "    medication_details = []\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        med_details = extract_medication_details(\n",
        "            row[\"Prescrption json\"], row[\"Clinic Name\"], row[\"Date\"]\n",
        "        )\n",
        "        medication_details.extend(med_details)\n",
        "\n",
        "    # Create a DataFrame from the medication details\n",
        "    medication_df = pd.DataFrame(\n",
        "        medication_details,\n",
        "        columns=[\n",
        "            \"Medicine Name\",\n",
        "            \"Duration\",\n",
        "            \"Morning\",\n",
        "            \"Afternoon\",\n",
        "            \"Evening\",\n",
        "            \"Night\",\n",
        "            \"Frequency\",\n",
        "            \"Clinic Name\",\n",
        "            \"Date\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    medication_df[\"Clinic Name\"] = medication_df[\"Clinic Name\"].fillna(\"Master Clinic\")\n",
        "    medication_df.dropna(inplace=True)\n",
        "    print(medication_df.head())\n",
        "    \n",
        "    return medication_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2g0hFYIu5P5-"
      },
      "outputs": [],
      "source": [
        "def parse_date(date_str):\n",
        "    date_str = date_str.replace(\" \", \"-\")\n",
        "    date_str = date_str.replace(\"/\", \"-\")\n",
        "    date_str = date_str.replace(\".\", \"-\")\n",
        "\n",
        "    formats = [\n",
        "        \"%Y-%m-%d\",\n",
        "        \"%d-%m-%Y\",\n",
        "        \"%m-%d-%Y\",\n",
        "        \"%Y-%b-%d\",\n",
        "        \"%d-%b-%Y\",\n",
        "        \"%b-%d-%Y\",\n",
        "        \"%Y-%B-%d\",\n",
        "        \"%d-%B-%Y\",\n",
        "        \"%B-%d-%Y\",\n",
        "    ]\n",
        "\n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            d = pd.to_datetime(date_str, format=fmt)\n",
        "            # If date is valid but falls on an invalid leap year date, continue to next format\n",
        "            if d.month == 2 and d.day == 29 and not cal.isleap(d.year):\n",
        "                continue\n",
        "            return d.strftime(\"%Y:%m:%d\")\n",
        "        except ValueError:\n",
        "            pass\n",
        "    return pd.NaT\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GG6Jdp2o5P5_"
      },
      "outputs": [],
      "source": [
        "medicine_to_include = [\n",
        "    \"Albendazole\",\n",
        "    \"Amoxycillin\",\n",
        "    \"Ascabiol\",\n",
        "    \"Azithromycin\",\n",
        "    \"Beclo + Clotri + Genta cream\",\n",
        "    \"Becosule\",\n",
        "    \"Calcium +\",\n",
        "    \"Cetrizine\",\n",
        "    \"Ciplox eye drops\",\n",
        "    \"Ciprofloxacin\",\n",
        "    \"Clotrimazole\",\n",
        "    \"Cough syrup\",\n",
        "    \"Diclofenac gel\",\n",
        "    \"Dicyclomine + Mefenamic acid\",\n",
        "    \"dicyclomine (10mg) + mefenamic acid (250mg)\",\n",
        "    \"norfloxacin (200mg) + tinidazole (300mg)\",\n",
        "    \"norflox\",\n",
        "    \"omez\",\n",
        "    \"vitamin-c chewable 500mg tablet\",\n",
        "    \"unizyme\",\n",
        "    \"becousles\",\n",
        "    \"omez\",\n",
        "    \"Doxycycline\",\n",
        "    \"Dulcoflex\",\n",
        "    \"Herbal cough\",\n",
        "    \"Iron\",\n",
        "    \"Liver tonic\",\n",
        "    \"Miconazole nitrate\",\n",
        "    \"Multivitamin\",\n",
        "    \"Norfloxacin + Tinidazole\",\n",
        "    \"Omeprazole\",\n",
        "    \"Ondansetron\",\n",
        "    \"ORS\",\n",
        "    \"Paracetamol\",\n",
        "    \"Aceclo + PCM\",\n",
        "    \"Fluconazole\",\n",
        "    \"Neurobion forte\",\n",
        "    \"Unienzyme\",\n",
        "    \"Tetmosol soap\",\n",
        "    \"Vitamin C\",\n",
        "    \"Prochlorperazine\",\n",
        "    \"PCM\",\n",
        "    \"Limcee\",\n",
        "    \"Ivermectin\",\n",
        "    \"Benedryil syrup\",\n",
        "    \"Amoxy\",\n",
        "    \"Metronidazole\",\n",
        "    \"Miconazole Nitrate cream\",\n",
        "    \"Pediatric Multivitamin Syp\",\n",
        "    \"Prochlorperazine stemetil md\",\n",
        "    \"Aceclo + PCM (SOS)\",\n",
        "    \"Fluconazole\",\n",
        "    \"Tetmasol Soap\",\n",
        "    \"Vitamin C Chewable\",\n",
        "    \"Waxonil Ear Drop\",\n",
        "    \"calcium 250 mg+vit d3\",\n",
        "]\n",
        "\n",
        "medicine_to_include_lower = [name.lower() for name in medicine_to_include]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AZ3AVQTr5P5_"
      },
      "outputs": [],
      "source": [
        "def xyz(medication_df):\n",
        "    filtered_df = medication_df[\n",
        "    medication_df[\"Medicine Name\"]\n",
        "    .str.lower()\n",
        "    .apply(lambda x: any(med_name in x for med_name in medicine_to_include_lower))\n",
        "]\n",
        "    return filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v_MsXrxU5P6A"
      },
      "outputs": [],
      "source": [
        "def categorize_medicine(row, medicines_to_include):\n",
        "    row_lower = row.lower().strip()\n",
        "\n",
        "    # Normalize spacing and format\n",
        "    row_lower = re.sub(r\"\\s+\", \" \", row_lower)\n",
        "\n",
        "    # Specific condition for ORS\n",
        "    if row_lower.startswith(\"ors\"):\n",
        "        return \"solution\"\n",
        "\n",
        "    for med in medicines_to_include:\n",
        "        med_pattern = re.escape(med)\n",
        "        if re.search(med_pattern, row_lower):\n",
        "            if any(\n",
        "                x in row_lower for x in [\"syrup\", \"ml\", \"sry\", \"srp\", \"(ml)\", \"syp\"]\n",
        "            ):\n",
        "                return \"syrup\"\n",
        "            elif any(\n",
        "                x in row_lower\n",
        "                for x in [\"mg\", \"tab\", \"tabs\", \"tablets\", \"caps\", \"capsule\", \"cap\"]\n",
        "            ):\n",
        "                return \"tab\"\n",
        "            elif any(x in row_lower for x in [\"drops\", \"drop\"]):\n",
        "                return \"drops\"\n",
        "            elif any(x in row_lower for x in [\"cream\", \"creams\"]):\n",
        "                return \"cream\"\n",
        "            elif any(x in row_lower for x in [\"ointment\", \"ointments\", \"ointmnt\"]):\n",
        "                return \"ointment\"\n",
        "            elif any(x in row_lower for x in [\"gels\", \"gel\"]):\n",
        "                return \"gel\"\n",
        "            elif any(x in row_lower for x in [\"soaps\", \"soap\"]):\n",
        "                return \"soap\"\n",
        "\n",
        "    # If no category matched, return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d5nnAnCG5P6A"
      },
      "outputs": [],
      "source": [
        "def match_and_replace_medicine_name(medicine_name, medicines_to_include):\n",
        "    for med in medicines_to_include:\n",
        "        med_pattern = re.compile(re.escape(med), re.IGNORECASE)\n",
        "        if med_pattern.search(medicine_name):\n",
        "            return med\n",
        "    return medicine_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dqTlr2cP5P6A"
      },
      "outputs": [],
      "source": [
        "mappings = {\n",
        "    \"unienzyme\": \"unizyme\",\n",
        "    \"becosule\": \"becousles\",\n",
        "    \"tetmasol soap\": \"tetmosol soap\",\n",
        "    \"vitamin c\": \"vitamin-c chewable 500mg tablet\",\n",
        "    \"amoxycillin\": \"amoxy\",\n",
        "    \"norflox\": \"norfloxacin (200mg) + tinidazole (300mg)\",\n",
        "    \"calcium +\": \"calcium 250 mg+vit d3\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BvN9gif15P6A"
      },
      "outputs": [],
      "source": [
        "def mno(filtered_df):\n",
        "    numeric_cols = [\"Duration\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n",
        "\n",
        "    # Convert columns to numeric data type\n",
        "    for col in numeric_cols:\n",
        "        filtered_df[col] = pd.to_numeric(filtered_df[col], errors=\"coerce\")\n",
        "\n",
        "    # Display the DataFrame with converted columns\n",
        "    filtered_df.info()\n",
        "    combined_df = filtered_df[filtered_df[\"Frequency\"] == \"d\"]\n",
        "    combined_df[\"Quantity\"] = (\n",
        "    combined_df[\"Morning\"]\n",
        "    + combined_df[\"Afternoon\"]\n",
        "    + combined_df[\"Evening\"]\n",
        "    + combined_df[\"Night\"]\n",
        "    )\n",
        "\n",
        "    combined_df[\"Total Requirement\"] = combined_df[\"Quantity\"] * combined_df[\"Duration\"]\n",
        "    syrup_columns = [\n",
        "    \"Multivitamin Syrup\",\n",
        "    \"Cough Syrup\",\n",
        "    \"Diclofenac Gel\",\n",
        "    ]  # Replace with your actual column names\n",
        "\n",
        "    def convert_values(row):\n",
        "        if row[\"Medicine Name\"] in syrup_columns:\n",
        "            return 2 if row[\"Total Requirement\"] > 30 else 1\n",
        "        return row[\"Total Requirement\"]\n",
        "\n",
        "\n",
        "    combined_df[\"Total Requirement\"] = combined_df.apply(convert_values, axis=1)\n",
        "    combined_df.sort_values(by=\"Date\")\n",
        "    combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date\"], format=\"%d-%m-%Y\")\n",
        "    combined_df = combined_df[combined_df[\"Date\"] >= \"2023-01-01\"]\n",
        "    return combined_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CmVhqHDM_fMw"
      },
      "outputs": [],
      "source": [
        "min_months_required = 16\n",
        "# Function to check if a group has sufficient pivot_table\n",
        "def has_sufficient_data(group):\n",
        "    unique_months = pd.to_datetime(group[\"Date\"]).dt.to_period(\"M\").nunique()\n",
        "      # print(unique_months)\n",
        "    return unique_months >= min_months_required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "veb-t_nx74LK"
      },
      "outputs": [],
      "source": [
        "def pivot(combined_df):\n",
        "\n",
        "    # Convert the 'Date' column to datetime format and extract year and month\n",
        "  combined_df[\"Year\"] = combined_df[\"Date\"].dt.year\n",
        "  combined_df[\"Month\"] = combined_df[\"Date\"].dt.strftime(\"%b\")\n",
        "\n",
        "  # Create the pivot table\n",
        "  pivot_table = combined_df.pivot_table(\n",
        "      values=\"Total Requirement\",\n",
        "      index=[\"Clinic Name\", \"Medicine Name\", \"Year\", \"Month\"],\n",
        "      aggfunc=\"sum\",\n",
        "      margins=True,\n",
        "      margins_name=\"Sum of Total Requirement\",\n",
        "  )\n",
        "  pivot_table = pivot_table.iloc[:-1]\n",
        "  pivot_table.reset_index(inplace=True)\n",
        "  pivot_table[\"Date\"] = pd.to_datetime(\n",
        "    pivot_table[\"Year\"].astype(str) + \"-\" + pivot_table[\"Month\"],\n",
        "    format=\"%Y-%b\",\n",
        "    errors=\"coerce\",\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Apply the function and filter the pivot_table\n",
        "  filtered_data = pivot_table.groupby([\"Clinic Name\", \"Medicine Name\"]).filter(\n",
        "      has_sufficient_data\n",
        "  )\n",
        "  unique_clinics = filtered_data[\"Clinic Name\"].unique()\n",
        "  print(len(unique_clinics))\n",
        "\n",
        "# Save the filtered pivot_table to a new CSV (optional)\n",
        "  return filtered_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "htazjrRG9apt"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encoding(filtered_data):\n",
        "\n",
        "  # Create label encoder objects\n",
        "  clinic_encoder = LabelEncoder()\n",
        "  medicine_encoder = LabelEncoder()\n",
        "\n",
        "  # Fit and transform the categorical variables\n",
        "  filtered_data[\"Clinic Name Encoded\"] = clinic_encoder.fit_transform(\n",
        "      filtered_data[\"Clinic Name\"]\n",
        "  )\n",
        "  filtered_data[\"Medicine Name Encoded\"] = medicine_encoder.fit_transform(\n",
        "      filtered_data[\"Medicine Name\"]\n",
        "  )\n",
        "\n",
        "  # Optionally, keep the mappings for reference\n",
        "  clinic_mapping = dict(\n",
        "      zip(clinic_encoder.classes_, clinic_encoder.transform(clinic_encoder.classes_))\n",
        "  )\n",
        "  medicine_mapping = dict(\n",
        "      zip(\n",
        "          medicine_encoder.classes_, medicine_encoder.transform(medicine_encoder.classes_)\n",
        "      )\n",
        "  )\n",
        "  # Convert Year and Month to numeric values if necessary\n",
        "  filtered_data[\"Year\"] = filtered_data[\"Year\"].astype(int)\n",
        "  # Month might already be in a numeric format (e.g., 'Jan' -> 1), but ensure it's consistent\n",
        "  month_mapping = {\n",
        "      \"Jan\": 1,\n",
        "      \"Feb\": 2,\n",
        "      \"Mar\": 3,\n",
        "      \"Apr\": 4,\n",
        "      \"May\": 5,\n",
        "      \"Jun\": 6,\n",
        "      \"Jul\": 7,\n",
        "      \"Aug\": 8,\n",
        "      \"Sep\": 9,\n",
        "      \"Oct\": 10,\n",
        "      \"Nov\": 11,\n",
        "      \"Dec\": 12,\n",
        "  }\n",
        "  filtered_data[\"Month\"] = filtered_data[\"Month\"].map(month_mapping)\n",
        "  return filtered_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q7xVQH0E-NyT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pT5ANLHj-QEB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,\n",
        "    AdaBoostRegressor,\n",
        "    GradientBoostingRegressor,\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from catboost import CatBoostRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MoVGmhDs-1qZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(X_train, y_train, X_test, y_test, models, param, random_state=42):\n",
        "    report = {}\n",
        "\n",
        "    for i in range(len(list(models))):\n",
        "        model = list(models.values())[i]\n",
        "        para = param[list(models.keys())[i]]\n",
        "\n",
        "        gs = GridSearchCV(model, para, cv=3)\n",
        "        gs.fit(X_train, y_train)\n",
        "\n",
        "        model.set_params(**gs.best_params_)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        test_model_score = r2_score(y_test, y_test_pred)\n",
        "        report[list(models.keys())[i]] = test_model_score\n",
        "\n",
        "\n",
        "    return report\n",
        "def automate_forecasting(filtered_data, start_index=0, end_index=None, random_state=42):\n",
        "    results = []\n",
        "    unique_clinics = filtered_data[\"Clinic Name\"].unique()\n",
        "    if end_index is None:\n",
        "        end_index = len(unique_clinics)\n",
        "\n",
        "    for idx, clinic in enumerate(\n",
        "        unique_clinics[start_index:end_index], start=start_index\n",
        "    ):\n",
        "        print(f\"Processing clinic {idx + 1} out of {len(unique_clinics)}\")\n",
        "        print(clinic)\n",
        "        clinic_data = filtered_data[filtered_data[\"Clinic Name\"] == clinic]\n",
        "\n",
        "        # Prepare training and testing data\n",
        "        train_data = clinic_data[\n",
        "            (clinic_data[\"Year\"] >= 2023)\n",
        "            & ((clinic_data[\"Year\"] < 2024) | (clinic_data[\"Month\"].isin([1, 2, 3,4])))\n",
        "        ].copy()\n",
        "        test_data = clinic_data[\n",
        "            (clinic_data[\"Year\"] == 2024) & (clinic_data[\"Month\"].isin([5]))\n",
        "        ].copy()\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(f\"Skipping clinic '{clinic}' due to missing data.\")\n",
        "            continue\n",
        "        X_train = train_data[\n",
        "            [\"Clinic Name Encoded\", \"Medicine Name Encoded\", \"Year\", \"Month\"]\n",
        "        ]\n",
        "        y_train = train_data[\"Total Requirement\"]\n",
        "        X_test = test_data[\n",
        "            [\"Clinic Name Encoded\", \"Medicine Name Encoded\", \"Year\", \"Month\"]\n",
        "        ]\n",
        "        y_test = test_data[\"Total Requirement\"]\n",
        "\n",
        "        # Define models and parameters\n",
        "        models = {\n",
        "            \"Random Forest\": RandomForestRegressor(random_state=random_state),\n",
        "            \"Decision Tree\": DecisionTreeRegressor(random_state=random_state),\n",
        "            \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "            \"Linear Regression\": LinearRegression(),\n",
        "            \"XGBRegressor\": xgb.XGBRegressor(random_state=random_state),\n",
        "            \"AdaBoost Regressor\": AdaBoostRegressor(random_state=random_state),\n",
        "            \"CatBoosting Regressor\": CatBoostRegressor(\n",
        "                verbose=False, random_seed=random_state\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        params = {\n",
        "            \"Decision Tree\": {\n",
        "                \"criterion\": [\n",
        "                    \"squared_error\",\n",
        "                    \"friedman_mse\",\n",
        "                    \"absolute_error\",\n",
        "                    \"poisson\",\n",
        "                ],\n",
        "                \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
        "                \"min_samples_split\": [2, 5, 10],\n",
        "                \"min_samples_leaf\": [1, 2, 4],\n",
        "            },\n",
        "            \"Random Forest\": {\"n_estimators\": [8, 16, 32, 64, 128, 256]},\n",
        "            \"Gradient Boosting\": {\n",
        "                \"learning_rate\": [0.1, 0.01, 0.05, 0.001],\n",
        "                \"subsample\": [0.6, 0.7, 0.75, 0.8, 0.85, 0.9],\n",
        "                \"n_estimators\": [8, 16, 32, 64, 128, 256],\n",
        "            },\n",
        "            \"Linear Regression\": {},\n",
        "            \"XGBRegressor\": {\n",
        "                \"learning_rate\": [0.1, 0.01, 0.05, 0.001],\n",
        "                \"n_estimators\": [8, 16, 32, 64, 128, 256],\n",
        "            },\n",
        "            \"AdaBoost Regressor\": {\n",
        "                \"learning_rate\": [0.1, 0.01, 0.5, 0.001],\n",
        "                \"n_estimators\": [8, 16, 32, 64, 128, 256],\n",
        "            },\n",
        "            \"CatBoosting Regressor\": {\n",
        "                \"depth\": [6, 8, 10],\n",
        "                \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "                \"iterations\": [30, 50, 100],\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Evaluate models and select the best one\n",
        "        model_report = evaluate_models(\n",
        "            X_train, y_train, X_test, y_test, models, params, random_state=random_state\n",
        "        )\n",
        "        best_model_score = max(sorted(model_report.values()))\n",
        "        best_model_name = list(model_report.keys())[\n",
        "            list(model_report.values()).index(best_model_score)\n",
        "        ]\n",
        "        best_model = models[best_model_name]\n",
        "        y_pred_test = best_model.predict(X_test)\n",
        "        y_pred_test = np.round(y_pred_test)\n",
        "        y_pred_test_list = y_pred_test.tolist()\n",
        "\n",
        "        # Retrain the best model on all available data before forecasting June 2024\n",
        "        X_all = filtered_data[\n",
        "            [\"Clinic Name Encoded\", \"Medicine Name Encoded\", \"Year\", \"Month\"]\n",
        "        ]\n",
        "        y_all = filtered_data[\"Total Requirement\"]\n",
        "        best_model.fit(X_all, y_all)\n",
        "\n",
        "        # Predicting with the best model for test data (April, May 2024)\n",
        "        if best_model_score > 0.4:\n",
        "\n",
        "\n",
        "            # Generate forecast for June 2024\n",
        "            june_2024_data = test_data.copy()\n",
        "            X_june_2024 = june_2024_data[\n",
        "                [\"Clinic Name Encoded\", \"Medicine Name Encoded\", \"Year\"]\n",
        "            ]\n",
        "            X_june_2024[\"Month\"] = 6\n",
        "            y_pred_june = best_model.predict(X_june_2024)\n",
        "            y_pred_june = np.round(y_pred_june)\n",
        "            y_pred_june_list = y_pred_june.tolist()\n",
        "\n",
        "            # Store the results\n",
        "            for idx, (actual, prediction_test, prediction_june) in enumerate(\n",
        "                zip(y_test, y_pred_test_list, y_pred_june_list)\n",
        "            ):\n",
        "                test_month = test_data.iloc[idx][\"Month\"]\n",
        "                test_year = test_data.iloc[idx][\"Year\"]\n",
        "                medicine_name = test_data.iloc[idx][\"Medicine Name\"]\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"Clinic Name\": clinic,\n",
        "                        \"Medicine Name\": medicine_name,\n",
        "                        f\"{test_month}/{test_year} Actual Data\": actual,\n",
        "                        f\"{test_month}/{test_year} Predicted Data\": prediction_test,\n",
        "                        \"June 2024 Forecasted Data\": prediction_june,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    # Convert results to a DataFrame for better visualization\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df\n",
        "\n",
        "    # Save results to a CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "uAsuwGcl5P6B",
        "outputId": "66692595-4cb4-4c63-8ebc-7af4161f3521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "********************************************************************************\n",
            "Processing channel: Namra Finance\n",
            "********************************************************************************\n",
            "Data shape: (446609, 4)\n",
            "Extracting Json Data\n",
            "                Medicine Name Duration  Morning  Afternoon  Evening  Night  \\\n",
            "0                 NORFLOX -TZ        3        1          0        1      0   \n",
            "1                  ACILOC- RD        5        1          0        1      0   \n",
            "2                Ondem 4 Tabs        3        1          1        0      1   \n",
            "3  Aceclofenac 100 mg Tablets        5        1          0        0      0   \n",
            "4             METROGYL 200 mg        5        1          1        1      0   \n",
            "\n",
            "  Frequency    Clinic Name        Date  \n",
            "0         d  Master Clinic  2022-04-25  \n",
            "1         d  Master Clinic  2022-04-25  \n",
            "2         d  Master Clinic  2022-04-25  \n",
            "3         d  Master Clinic  2022-04-28  \n",
            "4         d  Master Clinic  2022-04-28  \n",
            "(1320079, 9)\n",
            "Parsing Data\n",
            "Converting medicines\n",
            "(1238709, 9)\n",
            "Standardizing them\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1441059231.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df[\"category\"] = filtered_df[\"Medicine Name\"].apply(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matching them\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1441059231.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df[\"Medicine Name\"] = filtered_df[\"Medicine Name\"].apply(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Mapping\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1441059231.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df[\"Medicine Name\"] = filtered_df[\"Medicine Name\"].replace(mappings)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Taking 2023 data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\904765211.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df[col] = pd.to_numeric(filtered_df[col], errors=\"coerce\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1238709 entries, 0 to 1320078\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count    Dtype         \n",
            "---  ------         --------------    -----         \n",
            " 0   Medicine Name  1238709 non-null  object        \n",
            " 1   Duration       1238709 non-null  int64         \n",
            " 2   Morning        1238709 non-null  int64         \n",
            " 3   Afternoon      1238709 non-null  int64         \n",
            " 4   Evening        1238709 non-null  int64         \n",
            " 5   Night          1238709 non-null  int64         \n",
            " 6   Frequency      1238709 non-null  object        \n",
            " 7   Clinic Name    1238709 non-null  object        \n",
            " 8   Date           1238709 non-null  datetime64[ns]\n",
            " 9   category       1218800 non-null  object        \n",
            "dtypes: datetime64[ns](1), int64(5), object(4)\n",
            "memory usage: 104.0+ MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\904765211.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  combined_df[\"Quantity\"] = (\n",
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\904765211.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  combined_df[\"Total Requirement\"] = combined_df[\"Quantity\"] * combined_df[\"Duration\"]\n",
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\904765211.py:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  combined_df[\"Total Requirement\"] = combined_df.apply(convert_values, axis=1)\n",
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\904765211.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date\"], format=\"%d-%m-%Y\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined DataFrame shape: (1118479, 12)\n",
            "Converting to month format\n",
            "142\n",
            "                  Clinic Name  Medicine Name  Year Month  Total Requirement  \\\n",
            "651    ECN001 Saidpur Gazipur          amoxy  2023   Apr                150   \n",
            "652    ECN001 Saidpur Gazipur          amoxy  2023   Aug                325   \n",
            "653    ECN001 Saidpur Gazipur          amoxy  2023   Dec                180   \n",
            "654    ECN001 Saidpur Gazipur          amoxy  2023   Feb                132   \n",
            "655    ECN001 Saidpur Gazipur          amoxy  2023   Jan                 74   \n",
            "...                       ...            ...   ...   ...                ...   \n",
            "87403           Master Clinic  tetmosol soap  2023   Sep                  2   \n",
            "87404           Master Clinic  tetmosol soap  2024   Apr                126   \n",
            "87405           Master Clinic  tetmosol soap  2024   Feb                 96   \n",
            "87406           Master Clinic  tetmosol soap  2024   Jan                  1   \n",
            "87407           Master Clinic  tetmosol soap  2024   May                  7   \n",
            "\n",
            "            Date  \n",
            "651   2023-04-01  \n",
            "652   2023-08-01  \n",
            "653   2023-12-01  \n",
            "654   2023-02-01  \n",
            "655   2023-01-01  \n",
            "...          ...  \n",
            "87403 2023-09-01  \n",
            "87404 2024-04-01  \n",
            "87405 2024-02-01  \n",
            "87406 2024-01-01  \n",
            "87407 2024-05-01  \n",
            "\n",
            "[28218 rows x 6 columns]\n",
            "Encoding\n",
            "Model Training\n",
            "Processing clinic 1 out of 142\n",
            "ECN001 Saidpur Gazipur\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 2 out of 142\n",
            "ECN002 BIJNOR BIJNOR\n",
            "Processing clinic 3 out of 142\n",
            "ECN003 Noorpur\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 4 out of 142\n",
            "ECN004 BIHTA PATNA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 5 out of 142\n",
            "ECN007 KHEDA KHEDA\n",
            "Processing clinic 6 out of 142\n",
            "ECN008 KATHLAL KHEDA\n",
            "Processing clinic 7 out of 142\n",
            "ECN009 PIPLOD DAHOD\n",
            "Processing clinic 8 out of 142\n",
            "ECN010 Pratapgarh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 9 out of 142\n",
            "ECN011 Dhamnod\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 10 out of 142\n",
            "ECN012 Roorkee\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 11 out of 142\n",
            "ECN013 Ichhawar \n",
            "Processing clinic 12 out of 142\n",
            "ECN014 Narayanpur\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 13 out of 142\n",
            "ECN015 Varanasi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\siidh\\AppData\\Local\\Temp\\ipykernel_30324\\1564421383.py:131: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_june_2024[\"Month\"] = 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clinic 14 out of 142\n",
            "ECN016 PIPRAICH\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#automating\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mautomate_forecasting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencodeed_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m result\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Prediction.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[23], line 103\u001b[0m, in \u001b[0;36mautomate_forecasting\u001b[1;34m(filtered_data, start_index, end_index, random_state)\u001b[0m\n\u001b[0;32m     68\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m     },\n\u001b[0;32m    100\u001b[0m }\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Evaluate models and select the best one\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m model_report \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m best_model_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28msorted\u001b[39m(model_report\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m    107\u001b[0m best_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_report\u001b[38;5;241m.\u001b[39mkeys())[\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mlist\u001b[39m(model_report\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mindex(best_model_score)\n\u001b[0;32m    109\u001b[0m ]\n",
            "Cell \u001b[1;32mIn[23], line 9\u001b[0m, in \u001b[0;36mevaluate_models\u001b[1;34m(X_train, y_train, X_test, y_test, models, param, random_state)\u001b[0m\n\u001b[0;32m      6\u001b[0m para \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;28mlist\u001b[39m(models\u001b[38;5;241m.\u001b[39mkeys())[i]]\n\u001b[0;32m      8\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(model, para, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgs\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\catboost\\core.py:5827\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5825\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5828\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5829\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5830\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\catboost\\core.py:2400\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2397\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2408\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2409\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\catboost\\core.py:1780\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
            "File \u001b[1;32m_catboost.pyx:4833\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_catboost.pyx:4882\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for channel in channels:\n",
        "    print(\"********************************************************************************\")\n",
        "    print(\"Processing channel:\", channel)\n",
        "    print(\"********************************************************************************\")\n",
        "    data = processed_df_1[[\"Date\", \"Clinic Name\", \"Channel\", \"Prescrption json\"]]\n",
        "    data = data[data[\"Channel\"].str.contains(channel, case=False, na=False)]\n",
        "    print(\"Data shape:\", data.shape)\n",
        "    print(\"Extracting Json Data\")\n",
        "    medication_df=abc(data)\n",
        "    print(medication_df.shape)\n",
        "    print(\"Parsing Data\")\n",
        "    medication_df[\"Date\"] = medication_df[\"Date\"].apply(parse_date)\n",
        "    medication_df[\"Date\"] = pd.to_datetime(medication_df[\"Date\"], format=\"%Y:%m:%d\")\n",
        "    medication_df[\"Medicine Name\"] = medication_df[\"Medicine Name\"].str.lower()\n",
        "    print(\"Converting medicines\")\n",
        "    filtered_df = xyz(medication_df)\n",
        "    print(filtered_df.shape)\n",
        "    print(\"Standardizing them\")\n",
        "    filtered_df[\"category\"] = filtered_df[\"Medicine Name\"].apply(\n",
        "    lambda row: categorize_medicine(row, medicines_to_include=medicine_to_include_lower)\n",
        "    )\n",
        "    print(\"Matching them\")\n",
        "    filtered_df[\"Medicine Name\"] = filtered_df[\"Medicine Name\"].apply(\n",
        "    lambda name: match_and_replace_medicine_name(name, medicine_to_include_lower)\n",
        "    )\n",
        "    print(\"Final Mapping\")\n",
        "    filtered_df[\"Medicine Name\"] = filtered_df[\"Medicine Name\"].replace(mappings)\n",
        "    print(\"Taking 2023 data\")\n",
        "    combined_df=mno(filtered_df)\n",
        "    print(\"Combined DataFrame shape:\", combined_df.shape)\n",
        "    print(\"Converting to month format\")\n",
        "    training_df=pivot(combined_df)\n",
        "    print(training_df)\n",
        "    print(\"Encoding\")\n",
        "    encodeed_df=encoding(training_df)\n",
        "    #automating\n",
        "    print(\"Model Training\")\n",
        "    result=automate_forecasting(encodeed_df)\n",
        "    result.to_csv(f\"{channel}_Prediction.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvVYdZu8ADMF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
